# 5 月 29 日

## ch8 并发

### 内核态的线程管理

#### 线程管理机制的设计与实现

os/src/syscall/thread.rs 中实现了创建线程的系统调用：

```rust
/// thread create syscall
pub fn sys_thread_create(entry: usize, arg: usize) -> isize {
    trace!(
        "kernel:pid[{}] tid[{}] sys_thread_create",
        current_task().unwrap().process.upgrade().unwrap().getpid(),
        current_task()
            .unwrap()
            .inner_exclusive_access()
            .res
            .as_ref()
            .unwrap()
            .tid
    );
    let task = current_task().unwrap();
    let process = task.process.upgrade().unwrap();
    // create a new thread
    let new_task = Arc::new(TaskControlBlock::new(
        Arc::clone(&process),
        task.inner_exclusive_access()
            .res
            .as_ref()
            .unwrap()
            .ustack_base,
        true,
    ));
    // add new task to scheduler
    add_task(Arc::clone(&new_task));
    let new_task_inner = new_task.inner_exclusive_access();
    let new_task_res = new_task_inner.res.as_ref().unwrap();
    let new_task_tid = new_task_res.tid;
    let mut process_inner = process.inner_exclusive_access();
    // add new thread to current process
    let tasks = &mut process_inner.tasks;
    while tasks.len() < new_task_tid + 1 {
        tasks.push(None);
    }
    tasks[new_task_tid] = Some(Arc::clone(&new_task));
    let new_task_trap_cx = new_task_inner.get_trap_cx();
    *new_task_trap_cx = TrapContext::app_init_context(
        entry,
        new_task_res.ustack_top(),
        kernel_token(),
        new_task.kstack.get_top(),
        trap_handler as usize,
    );
    (*new_task_trap_cx).x[10] = arg;
    new_task_tid as isize
}
```

此函数会修改当前任务中 process 对应的 tasks（任务（线程）控制块原子引用计数向量）以加入进程控制块，该向量对应的下标即为线程的 tid。其中，在 os/src/task/task.rs 中如下的任务控制块的 ```new``` 函数被调用：

```rust
impl TaskControlBlock {
    /// Create a new task
    pub fn new(
        process: Arc<ProcessControlBlock>,
        ustack_base: usize,
        alloc_user_res: bool,
    ) -> Self {
        let res = TaskUserRes::new(Arc::clone(&process), ustack_base, alloc_user_res);
        let trap_cx_ppn = res.trap_cx_ppn();
        let kstack = kstack_alloc();
        let kstack_top = kstack.get_top();
        Self {
            process: Arc::downgrade(&process),
            kstack,
            inner: unsafe {
                UPSafeCell::new(TaskControlBlockInner {
                    res: Some(res),
                    trap_cx_ppn,
                    task_cx: TaskContext::goto_trap_return(kstack_top),
                    task_status: TaskStatus::Ready,
                    exit_code: None,
                })
            },
        }
    }
}
```

这里多了个原来不曾出现的参数 ustack_base，该参数在 sys_thread_create 中被设为 ```task.inner_exclusive_access().res.as_ref().unwrap().ustack_base```，即从当前线程的线程控制块读取，与当前线程一致。该参数的用处将在稍后设置用户栈时揭晓。

这里关于 TrapContext 的处理有所变化，之前作为进程控制块时其代码如下：

```rust
        let trap_cx_ppn = memory_set
            .translate(VirtAddr::from(TRAP_CONTEXT_BASE).into())
            .unwrap()
            .ppn();
```

现在其调用了 TaskUserRes 的 ```trap_cx_ppn()``` 函数，定义在 os/src/task/id.rs 中：

```rust
impl TaskUserRes {
    /// Create a new TaskUserRes (Task User Resource)
    pub fn new(
        process: Arc<ProcessControlBlock>,
        ustack_base: usize,
        alloc_user_res: bool,
    ) -> Self {
        let tid = process.inner_exclusive_access().alloc_tid();
        let task_user_res = Self {
            tid,
            ustack_base,
            process: Arc::downgrade(&process),
        };
        if alloc_user_res {
            task_user_res.alloc_user_res();
        }
        task_user_res
    }
    // ...
    /// The physical page number(ppn) of the trap context for a task with tid
    pub fn trap_cx_ppn(&self) -> PhysPageNum {
        let process = self.process.upgrade().unwrap();
        let process_inner = process.inner_exclusive_access();
        let trap_cx_bottom_va: VirtAddr = trap_cx_bottom_from_tid(self.tid).into();
        process_inner
            .memory_set
            .translate(trap_cx_bottom_va.into())
            .unwrap()
            .ppn()
    }
    // ...
}
```

为了弄清 TrapContext 和用户栈具体的变化，我们先从 TaskUserRes 的 new 开始看。这个函数构造了一个 TaskUserRes，其中 tid 用 ```alloc_tid``` 分配，这个函数在 os/src/task/process.rs 中：

```rust
impl ProcessControlBlockInner {
    // ...
    /// allocate a new task id
    pub fn alloc_tid(&mut self) -> usize {
        self.task_res_allocator.alloc()
    }
    /// deallocate a task id
    pub fn dealloc_tid(&mut self, tid: usize) {
        self.task_res_allocator.dealloc(tid)
    }
    // ...
}
```

其作用就是借助 RecycleAllocator 分配一个 tid 号码。

由于我们在 new 时给参数 alloc_user_res 赋值为 true，因此调用了 alloc_user_res 函数，就是该函数设置了该线程的用户栈与 TrapContext：

```rust
impl TaskUserRes {
    // ...
    /// Allocate user resource for a task
    pub fn alloc_user_res(&self) {
        let process = self.process.upgrade().unwrap();
        let mut process_inner = process.inner_exclusive_access();
        // alloc user stack
        let ustack_bottom = ustack_bottom_from_tid(self.ustack_base, self.tid);
        let ustack_top = ustack_bottom + USER_STACK_SIZE;
        process_inner.memory_set.insert_framed_area(
            ustack_bottom.into(),
            ustack_top.into(),
            MapPermission::R | MapPermission::W | MapPermission::U,
        );
        // alloc trap_cx
        let trap_cx_bottom = trap_cx_bottom_from_tid(self.tid);
        let trap_cx_top = trap_cx_bottom + PAGE_SIZE;
        process_inner.memory_set.insert_framed_area(
            trap_cx_bottom.into(),
            trap_cx_top.into(),
            MapPermission::R | MapPermission::W,
        );
    }
    // ...
}
```

这里，ustack_bottom_from_tid 函数用来根据 ustack_base 以及 tid 计算出用户栈页面的底部的地址，可以看出计算时为每个线程的用户栈和保护页面预留空间。同样，trap_cx_bottom_from_tid 函数用来根据 tid 计算出 TrapContext 的基地址（由于 TrapContext 在整个用户地址空间的最高端（仅次于 Trampoline），是向下排列的）：

```rust
/// Return the bottom addr (low addr) of the trap context for a task
fn trap_cx_bottom_from_tid(tid: usize) -> usize {
    TRAP_CONTEXT_BASE - tid * PAGE_SIZE
}
/// Return the bottom addr (high addr) of the user stack for a task
fn ustack_bottom_from_tid(ustack_base: usize, tid: usize) -> usize {
    ustack_base + tid * (PAGE_SIZE + USER_STACK_SIZE)
}
```

之后，用户栈和 TrapContext 对应的逻辑段被创建并插入地址空间。完成了这些后，sys_thread_create 函数就可以在后面根据函数参数中的线程入口点地址和参数写入 TrapContext 了。

退出当前线程的系统调用仍然是 os/src/syscall/process.rs 中的 sys_exit：

```rust
/// exit syscall
///
/// exit the current task and run the next task in task list
pub fn sys_exit(exit_code: i32) -> ! {
    trace!(
        "kernel:pid[{}] sys_exit",
        current_task().unwrap().process.upgrade().unwrap().getpid()
    );
    exit_current_and_run_next(exit_code);
    panic!("Unreachable in sys_exit!");
}
```

其中，os/src/task/mod.rs 中的 exit_current_and_run_next 函数与前面 ch5 中实现的版本相比有变化：

```rust
/// Exit the current 'Running' task and run the next task in task list.
pub fn exit_current_and_run_next(exit_code: i32) {
    trace!(
        "kernel: pid[{}] exit_current_and_run_next",
        current_task().unwrap().process.upgrade().unwrap().getpid()
    );
    // take from Processor
    let task = take_current_task().unwrap();
    let mut task_inner = task.inner_exclusive_access();
    let process = task.process.upgrade().unwrap();
    let tid = task_inner.res.as_ref().unwrap().tid;
    // record exit code
    task_inner.exit_code = Some(exit_code);
    task_inner.res = None;
    // here we do not remove the thread since we are still using the kstack
    // it will be deallocated when sys_waittid is called
    drop(task_inner);

    // Move the task to stop-wait status, to avoid kernel stack from being freed
    if tid == 0 {
        add_stopping_task(task);
    } else {
        drop(task);
    }
    // however, if this is the main thread of current process
    // the process should terminate at once
    if tid == 0 {
        let pid = process.getpid();
        if pid == IDLE_PID {
            println!(
                "[kernel] Idle process exit with exit_code {} ...",
                exit_code
            );
            if exit_code != 0 {
                //crate::sbi::shutdown(255); //255 == -1 for err hint
                crate::board::QEMU_EXIT_HANDLE.exit_failure();
            } else {
                //crate::sbi::shutdown(0); //0 for success hint
                crate::board::QEMU_EXIT_HANDLE.exit_success();
            }
        }
        remove_from_pid2process(pid);
        let mut process_inner = process.inner_exclusive_access();
        // mark this process as a zombie process
        process_inner.is_zombie = true;
        // record exit code of main process
        process_inner.exit_code = exit_code;

        {
            // move all child processes under init process
            let mut initproc_inner = INITPROC.inner_exclusive_access();
            for child in process_inner.children.iter() {
                child.inner_exclusive_access().parent = Some(Arc::downgrade(&INITPROC));
                initproc_inner.children.push(child.clone());
            }
        }

        // deallocate user res (including tid/trap_cx/ustack) of all threads
        // it has to be done before we dealloc the whole memory_set
        // otherwise they will be deallocated twice
        let mut recycle_res = Vec::<TaskUserRes>::new();
        for task in process_inner.tasks.iter().filter(|t| t.is_some()) {
            let task = task.as_ref().unwrap();
            // if other tasks are Ready in TaskManager or waiting for a timer to be
            // expired, we should remove them.
            //
            // Mention that we do not need to consider Mutex/Semaphore since they
            // are limited in a single process. Therefore, the blocked tasks are
            // removed when the PCB is deallocated.
            trace!("kernel: exit_current_and_run_next .. remove_inactive_task");
            remove_inactive_task(Arc::clone(&task));
            let mut task_inner = task.inner_exclusive_access();
            if let Some(res) = task_inner.res.take() {
                recycle_res.push(res);
            }
        }
        // dealloc_tid and dealloc_user_res require access to PCB inner, so we
        // need to collect those user res first, then release process_inner
        // for now to avoid deadlock/double borrow problem.
        drop(process_inner);
        recycle_res.clear();

        let mut process_inner = process.inner_exclusive_access();
        process_inner.children.clear();
        // deallocate other data in user space i.e. program code/data section
        process_inner.memory_set.recycle_data_pages();
        // drop file descriptors
        process_inner.fd_table.clear();
        // remove all tasks
        process_inner.tasks.clear();
    }
    drop(process);
    // we do not have to save task context
    let mut _unused = TaskContext::zero_init();
    schedule(&mut _unused as *mut _);
}
```

该函数首先填入 exit_code，然后清空 TaskControlBlockInner 中的 ```res```（个人觉得似乎没必要），然后将带锁的资源释放。如果 tid 等于 0，即该函数在主线程中被调用，那么我们把线程控制块的原子引用计数的所有权直接用 ```add_stopping_task``` 移走，否则将其释放。tid 等于 0 时，如果当前进程是 IDLE，那么需要退出 QEMU 了。否则，将其 pid 用 remove_from_pid2process 函数从 PDI2PCB 数据结构中移走，把进程设为僵尸，把其子进程的父进程设为 INIT 并将子进程列表清空，对每个线程调用 remove_inactive_task，将每个线程的 res 清除，最后回收页表，清除文件描述符，清除所有线程，用 ```schedule``` 调度。

add_stopping_task 在 os/src/task/manager.rs 中定义，实质上把任务控制块的所有权转移到了 TASK_MANAGER 中，相比 ch5 中的 TaskManager，这里的 TaskManager 的 ```stop_task``` 是新增的。

```rust
///A array of `TaskControlBlock` that is thread-safe
pub struct TaskManager {
    ready_queue: VecDeque<Arc<TaskControlBlock>>,
    
    /// The stopping task, leave a reference so that the kernel stack will not be recycled when switching tasks
    stop_task: Option<Arc<TaskControlBlock>>,
}

/// A simple FIFO scheduler.
impl TaskManager {
    // ...
    /// Add a task to stopping task
    pub fn add_stop(&mut self, task: Arc<TaskControlBlock>) {
        // NOTE: as the last stopping task has completely stopped (not
        // using kernel stack any more, at least in the single-core
        // case) so that we can simply replace it;
        self.stop_task = Some(task);
    }

}
// ...
/// Set a task to stop-wait status, waiting for its kernel stack out of use.
pub fn add_stopping_task(task: Arc<TaskControlBlock>) {
    TASK_MANAGER.exclusive_access().add_stop(task);
}
```

这个 stop_task 唯一的作用就是临时拥有主线程的所有权，这样做的原因是主线程的线程控制块中有个成员 ```pub kstack: KernelStack```，主线程在 drop 的时候会把内核栈给回收掉，因此我们没法在 exit_current_and_run_next 中把它 drop 掉，就需要单独把该进程控制块挪走，等下次有某个进程的主线程退出时把它顶替掉，这时再回收内核栈。为什么其他线程退出时不需要呢？我认为从代码上看，其他线程退出时其线程控制块根本就没删除。那怎么保证今后不会调度到该线程？其实正在运行的进程本就不在 ready_queue 里，对比 suspend_current_and_run_next 可知，如果该线程以后还需要继续被调度的话，需要调用 ```add_task(task)```，而这里并没有，因此该线程不可能再被调度。

os/src/task/manager.rs 中借助 BTreeMap 实现了数据结构 PID2PCB，将 PID 映射到进程控制块：

```rust
lazy_static! {
    // ...
    /// PID2PCB instance (map of pid to pcb)
    pub static ref PID2PCB: UPSafeCell<BTreeMap<usize, Arc<ProcessControlBlock>>> =
        unsafe { UPSafeCell::new(BTreeMap::new()) };
}
// ...
/// Remove item(pid, _some_pcb) from PDI2PCB map (called by exit_current_and_run_next)
pub fn remove_from_pid2process(pid: usize) {
    let mut map = PID2PCB.exclusive_access();
    if map.remove(&pid).is_none() {
        panic!("cannot find pid {} in pid2task!", pid);
    }
}
```

os/src/task/mod.rs 中定义了 remove_inactive_task 函数：

```rust
/// the inactive(blocked) tasks are removed when the PCB is deallocated.(called by exit_current_and_run_next)
pub fn remove_inactive_task(task: Arc<TaskControlBlock>) {
    remove_task(Arc::clone(&task));
    trace!("kernel: remove_inactive_task .. remove_timer");
    remove_timer(Arc::clone(&task));
}
```

os/src/task/manager.rs 中的 remove_task 函数从任务管理器中移除该任务：

```rust
/// Remove a task from the ready queue
pub fn remove_task(task: Arc<TaskControlBlock>) {
    //trace!("kernel: TaskManager::remove_task");
    TASK_MANAGER.exclusive_access().remove(task);
}
```

对应的代码如下：

```rust
impl TaskManager {
    // ...
    pub fn remove(&mut self, task: Arc<TaskControlBlock>) {
        if let Some((id, _)) = self
            .ready_queue
            .iter()
            .enumerate()
            .find(|(_, t)| Arc::as_ptr(t) == Arc::as_ptr(&task))
        {
            self.ready_queue.remove(id);
        }
    }
    // ...
}
```

而 remove_timer 在 os/src/timer.rs 中，这部分代码在本章中有较大改变。由于 rCore-Tutorial-Guide-2023S 和 rCore-Tutorial-Book-v3 3.6.0-alpha.1 文档都没提到这个事，加上我们时间紧张，就先不管了。

os/src/syscall/thread.rs 还实现了 sys_waittid 系统调用：

```rust
/// wait for a thread to exit syscall
///
/// thread does not exist, return -1
/// thread has not exited yet, return -2
/// otherwise, return thread's exit code
pub fn sys_waittid(tid: usize) -> i32 {
    trace!(
        "kernel:pid[{}] tid[{}] sys_waittid",
        current_task().unwrap().process.upgrade().unwrap().getpid(),
        current_task()
            .unwrap()
            .inner_exclusive_access()
            .res
            .as_ref()
            .unwrap()
            .tid
    );
    let task = current_task().unwrap();
    let process = task.process.upgrade().unwrap();
    let task_inner = task.inner_exclusive_access();
    let mut process_inner = process.inner_exclusive_access();
    // a thread cannot wait for itself
    if task_inner.res.as_ref().unwrap().tid == tid {
        return -1;
    }
    let mut exit_code: Option<i32> = None;
    let waited_task = process_inner.tasks[tid].as_ref();
    if let Some(waited_task) = waited_task {
        if let Some(waited_exit_code) = waited_task.inner_exclusive_access().exit_code {
            exit_code = Some(waited_exit_code);
        }
    } else {
        // waited thread does not exist
        return -1;
    }
    if let Some(exit_code) = exit_code {
        // dealloc the exited thread
        process_inner.tasks[tid] = None;
        exit_code
    } else {
        // waited thread has not exited
        -2
    }
}
```

搞明白 exit_current_and_run_next 中线程控制块不退出的问题，以上函数就容易理解了，其从线程控制块中对应的位置拿出 ```exit_code``` 并将线程控制块销毁。

## 总结

实事求是地讲，看到题目“并发”，我还以为要让 OS 在多核上运行。看来要一步一步地来，先把线程的理念搞懂。现在觉得一边看一边记录比较有用，方便事后查找。明天继续学习 rCore。